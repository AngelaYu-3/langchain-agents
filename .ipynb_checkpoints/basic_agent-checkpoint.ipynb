{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0f792b-bc5c-46c5-8acb-c15e11039ef9",
   "metadata": {},
   "source": [
    "### environment setup (langsmith and tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7694c74e-d286-49bf-96fd-62a141f8ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All environment variables with 'LANG' or 'TAVILY':\n",
      "LANG: en_US.UTF-...\n",
      "LANGSMITH_API_KEY: lsv2_pt_68...\n",
      "TAVILY_API_KEY: tvly-dev-j...\n",
      "\n",
      "LangSmith key exists: True\n",
      "Tavily key exists: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override = True)\n",
    "\n",
    "# Check what's actually in the environment\n",
    "print(\"All environment variables with 'LANG' or 'TAVILY':\")\n",
    "for key, value in os.environ.items():\n",
    "    if 'LANG' in key.upper() or 'TAVILY' in key.upper():\n",
    "        print(f\"{key}: {value[:10]}...\")\n",
    "\n",
    "# Check specifically for our keys\n",
    "langsmith_key = os.environ.get(\"LANGSMITH_API_KEY\")\n",
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "\n",
    "print(f\"\\nLangSmith key exists: {langsmith_key is not None}\")\n",
    "print(f\"Tavily key exists: {tavily_key is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b8b32-3327-4c09-a7a9-f75d1fd11827",
   "metadata": {},
   "source": [
    "### defining tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72bd3a9a-9774-47e5-b56d-d2f455e3ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the weather in SF', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Weather in San Francisco, CA', 'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750878802, 'localtime': '2025-06-25 12:13'}, 'current': {'last_updated_epoch': 1750878000, 'last_updated': '2025-06-25 12:00', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 8.7, 'wind_kph': 14.0, 'wind_degree': 240, 'wind_dir': 'WSW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 75, 'cloud': 50, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'windchill_c': 11.0, 'windchill_f': 51.8, 'heatindex_c': 12.2, 'heatindex_f': 54.0, 'dewpoint_c': 11.0, 'dewpoint_f': 51.7, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 8.4, 'gust_mph': 10.4, 'gust_kph': 16.8}}\", 'score': 0.9106173, 'raw_content': None}, {'title': 'Weather in San Francisco in June 2025', 'url': 'https://world-weather.info/forecast/usa/san_francisco/june-2025/', 'content': \"Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63° +55° *   2 +66° +54° *   3 +66° +55° *   4 +66° +54° *   5 +66° +55° *   6 +66° +57° *   7 +64° +55° *   8 +63° +55° *   9 +63° +54° *   10 +59° +54° *   11 +59° +54° *   12 +61° +54° *   13 +63° +54° Weather in Washington, D.C.**+75°** Redwood City**+59°** San Leandro**+57°** San Mateo**+55°** San Rafael**+59°** San Ramon**+55°** South San Francisco**+55°** Pacifica**+52°** Daly City**+54°** world's temperature today Goulburn day+50°F night+23°F Rustaq day+117°F night+93°F Weather forecast on your site Install _San Francisco_ +55° Temperature units\", 'score': 0.85995835, 'raw_content': None}], 'response_time': 2.02}\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "search = TavilySearch(max_results=2)\n",
    "search_results = search.invoke(\"What is the weather in SF\")\n",
    "print(search_results)\n",
    "\n",
    "# If we want, we can create other tools.\n",
    "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77862e0-aff4-4a4b-a1a4-7f4dcf6fd315",
   "metadata": {},
   "source": [
    "### using language model & integrating tools (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41fd72-946d-405f-85e3-731319fb92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f02c5ea-5796-4205-b22b-1c5b6d1e1042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Hi! How are you?\n",
      "Response:\n",
      "This was a bit difficult to read. I can see that you're a programmer but you could also use a GUI to\n",
      "\n",
      "\n",
      "Query:\n",
      "What's the weather in SF?\n",
      "Response:\n",
      "**Result 1:**\n",
      "Title: Weather in San Francisco, CA\n",
      "URL: https://www.weatherapi.com/\n",
      "Summary: {'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1750881249, 'l...\n",
      "\n",
      "**Result 2:**\n",
      "Title: Weather in San Francisco in June 2025\n",
      "URL: https://world-weather.info/forecast/usa/san_francisco/june-2025/\n",
      "Summary: Weather in San Francisco in June 2025 (California) - Detailed Weather Forecast for a Month *   Weather in San Francisco Weather in San Francisco in June 2025 *   1 +63° +55° *   2 +66° +54° *   3 +66°...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Query:\n",
      "Search for Python tutorials\n",
      "Response:\n",
      "**Result 1:**\n",
      "Title: Python Tutorial - W3Schools\n",
      "URL: https://www.w3schools.com/python/\n",
      "Summary: Python Tutorial Tutorials \"Tutorials and References\")Exercises \"Exercises and Quizzes\")Certificates \"Certificates\")Services \"Our Services\")Menu \"Menu\") Learn ChatGPT-3.5Tutorial Learn ChatGPT-4Tutoria...\n",
      "\n",
      "**Result 2:**\n",
      "Title: The Python Tutorial — Python 3.13.5 documentation\n",
      "URL: https://docs.python.org/3/tutorial/index.html\n",
      "Summary: The Python Tutorial The same site also contains distributions of and pointers to many free third party Python modules, programs and tools, and additional documentation. The Python interpreter is easil...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "class LocalChatModel:\n",
    "    \"\"\" Constructor \"\"\"\n",
    "    def __init__(self):\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\", \n",
    "            model=\"gpt2\",\n",
    "            pad_token_id=50256  # avoid warnings\n",
    "        )\n",
    "\n",
    "        self.tools = []\n",
    "        self.tavily = TavilySearch(max_results=2)\n",
    "\n",
    "    \"\"\" Format Tavily results nicely \"\"\"\n",
    "    def format_tavily_results(self, search_results):\n",
    "        if not search_results or 'results' not in search_results:\n",
    "            return \"No results found.\"\n",
    "\n",
    "        formatted = \"\"\n",
    "    \n",
    "        for i, result in enumerate(search_results['results'], 1):\n",
    "            formatted += f\"**Result {i}:**\\n\"\n",
    "            formatted += f\"Title: {result['title']}\\n\"\n",
    "            formatted += f\"URL: {result['url']}\\n\"\n",
    "            formatted += f\"Summary: {result['content'][:200]}...\\n\\n\"\n",
    "    \n",
    "        return formatted\n",
    "\n",
    "    \"\"\" Returning response based on query using gpt-2 model \"\"\"\n",
    "    def invoke(self, query):\n",
    "        # extract query message\n",
    "        if isinstance(query, list):\n",
    "            user_message = query[-1]['content']  # extract from Tavily tool message format\n",
    "        else:\n",
    "            user_message = query  # already a string\n",
    "            \n",
    "        # check if search is needed based on words in query message\n",
    "        needs_search = 'weather' in user_message.lower() or 'search' in user_message.lower()\n",
    "\n",
    "        if needs_search:\n",
    "            # go straight to Tavily, skip GPT-2 entirely\n",
    "            search_results = self.tavily.invoke(user_message)\n",
    "            response_content = self.format_tavily_results(search_results)\n",
    "        else:\n",
    "            # only use GPT-2 for non-search queries\n",
    "            prompt = f\"user says: {user_message}\\nResponse:\"\n",
    "\n",
    "            response = self.generator(\n",
    "                prompt,                  # given model context\n",
    "                max_new_tokens=25,       # generate max 25 new words       \n",
    "                temperature=0.8,         # creativity level (0=boring, 1=wild)     \n",
    "                do_sample=True,          # use randomness (not always same answer)\n",
    "                truncation=True,         # cut off if input too long\n",
    "                num_return_sequences=1   # generate 1 response (not multiple)\n",
    "            )\n",
    "\n",
    "            generated_text = response[0]['generated_text']\n",
    "            response_part = generated_text.split(\"Response:\")[-1].strip()\n",
    "\n",
    "            # rest of cleanup...\n",
    "            if '\\n' in response_part:\n",
    "                # only getting one sentence\n",
    "                response_part = response_part.split('\\n')[0]\n",
    "            if not response_part or len(response_part) < 3:\n",
    "                # no response, or reponse is less than 3 characters\n",
    "                response_part = \"Sorry, I didn't generate a good response. Please rephrase query.\"\n",
    "            response_content = response_part\n",
    "\n",
    "        return type('Response', (), {'content': response_content, 'user_message': user_message})()\n",
    "\n",
    "\n",
    "\"\"\" Testing \"\"\"\n",
    "model = LocalChatModel()\n",
    "test_queries = [\n",
    "    [{\"role\": \"user\", \"content\": \"Hi! How are you?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}], \n",
    "    [{\"role\": \"user\", \"content\": \"Search for Python tutorials\"}]\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response = model.invoke(query)\n",
    "    print(f\"Query:\\n{response.user_message}\\nResponse:\\n{response.content}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9771889-2909-4f41-941c-0b7339f822ae",
   "metadata": {},
   "source": [
    "### using language models & memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8c03229-07d2-4077-938e-1062ced7faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ALICE's CHAT\n",
      "============================================================\n",
      "Alice Turn 1:\n",
      "Agent sees 1 messages in memory:\n",
      "   1. Human: Hi, my name is Alice and I live in New York\n",
      "\n",
      "Alice Turn 2:\n",
      "Agent sees 3 messages in memory:\n",
      "   1. Human: Hi, my name is Alice and I live in New York\n",
      "   2. AI: Hello? My Name Is... Me! Bot. [Laughter] Wow was she cute but..\n",
      "   3. Human: What's my name?\n",
      "\n",
      "Alice Turn 3:\n",
      "Agent sees 5 messages in memory:\n",
      "   1. Human: Hi, my name is Alice and I live in New York\n",
      "   2. AI: Hello? My Name Is... Me! Bot. [Laughter] Wow was she cute but..\n",
      "   3. Human: What's my name?\n",
      "   4. AI: I'm in the company of a man named \"The Man on Fire\". He killed his wife and\n",
      "   5. Human: Where do I live?\n",
      "\n",
      "\n",
      "============================================================\n",
      "BOB'S CHAT\n",
      "============================================================\n",
      "Bob Turn 1:\n",
      "Agent sees 1 messages in memory:\n",
      "   1. Human: Hello, I'm Bob and I work as a teacher\n",
      "\n",
      "Bob Turn 2:\n",
      "Agent sees 3 messages in memory:\n",
      "   1. Human: Hello, I'm Bob and I work as a teacher\n",
      "   2. AI: Hi! My name is Tim of The Daily Beast. What are you doing here? (pause)\n",
      "   3. Human: What's my job?\n",
      "\n",
      "Bob Turn 3:\n",
      "Agent sees 5 messages in memory:\n",
      "   1. Human: Hello, I'm Bob and I work as a teacher\n",
      "   2. AI: Hi! My name is Tim of The Daily Beast. What are you doing here? (pause)\n",
      "   3. Human: What's my job?\n",
      "   4. AI: I think you're the worst, because if they'd just tell him what to do he would probably\n",
      "   5. Human: Do you remember my name?\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "BACK TO ALICE'S CONVERSATION\n",
      "============================================================\n",
      "Alice Turn 4 (returning to her thread):\n",
      "Agent sees 7 messages in memory:\n",
      "   1. Human: Hi, my name is Alice and I live in New York\n",
      "   2. AI: Hello? My Name Is... Me! Bot. [Laughter] Wow was she cute but..\n",
      "   3. Human: What's my name?\n",
      "   4. AI: I'm in the company of a man named \"The Man on Fire\". He killed his wife and\n",
      "   5. Human: Where do I live?\n",
      "   6. AI: http://www.reddit-movies.com/comments/_yq8Xzwj\n",
      "   7. Human: How many times have we talked?\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "CROSS-CONTAMINATION TEST\n",
      "============================================================\n",
      "Alice asks about Bob (should NOT know Bob):\n",
      "Agent sees 9 messages in memory:\n",
      "   1. Human: Hi, my name is Alice and I live in New York\n",
      "   2. AI: Hello? My Name Is... Me! Bot. [Laughter] Wow was she cute but..\n",
      "   3. Human: What's my name?\n",
      "   4. AI: I'm in the company of a man named \"The Man on Fire\". He killed his wife and\n",
      "   5. Human: Where do I live?\n",
      "   6. AI: http://www.reddit-movies.com/comments/_yq8Xzwj\n",
      "   7. Human: How many times have we talked?\n",
      "   8. AI: The only thing that's clear is the same old question. Do you think everyone thinks every company has\n",
      "   9. Human: Do you know anyone named Bob?\n",
      "\n",
      "Bob asks about Alice (should NOT know Alice):\n",
      "Agent sees 7 messages in memory:\n",
      "   1. Human: Hello, I'm Bob and I work as a teacher\n",
      "   2. AI: Hi! My name is Tim of The Daily Beast. What are you doing here? (pause)\n",
      "   3. Human: What's my job?\n",
      "   4. AI: I think you're the worst, because if they'd just tell him what to do he would probably\n",
      "   5. Human: Do you remember my name?\n",
      "   6. AI: Yes, I did. No questions asked! What is your last card number here?\n",
      "   7. Human: Do you know anyone named Alice?\n",
      "\n",
      "\n",
      "============================================================\n",
      "FINAL CONVERSATION STATES\n",
      "============================================================\n",
      "\n",
      "Alice's conversation has 10 messages:\n",
      "   1. Human: Hi, my name is Alice and I live in New York...\n",
      "   2. AI: Hello? My Name Is... Me! Bot. [Laughter] Wow was s...\n",
      "   3. Human: What's my name?...\n",
      "   4. AI: I'm in the company of a man named \"The Man on Fire...\n",
      "   5. Human: Where do I live?...\n",
      "   6. AI: http://www.reddit-movies.com/comments/_yq8Xzwj...\n",
      "   7. Human: How many times have we talked?...\n",
      "   8. AI: The only thing that's clear is the same old questi...\n",
      "   9. Human: Do you know anyone named Bob?...\n",
      "   10. AI: No, not yet. We'll be waiting for the first person...\n",
      "\n",
      "Bob's conversation has 8 messages:\n",
      "   1. Human: Hello, I'm Bob and I work as a teacher...\n",
      "   2. AI: Hi! My name is Tim of The Daily Beast. What are yo...\n",
      "   3. Human: What's my job?...\n",
      "   4. AI: I think you're the worst, because if they'd just t...\n",
      "   5. Human: Do you remember my name?...\n",
      "   6. AI: Yes, I did. No questions asked! What is your last ...\n",
      "   7. Human: Do you know anyone named Alice?...\n",
      "   8. AI: Yes. I can't remember her name, but the only thing...\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "KEY INSIGHTS FROM THIS TEST\n",
      "============================================================\n",
      "1. Each thread_id maintains completely separate conversation history\n",
      "2. Alice's conversation never knows about Bob's conversation\n",
      "3. Bob's conversation never knows about Alice's conversation\n",
      "4. You can return to any conversation thread and it remembers everything\n",
      "5. Each thread grows independently (different message counts)\n",
      "6. Thread isolation prevents data leakage between conversations\n",
      "\n",
      "CONVERSATION LENGTHS:\n",
      "   Alice: 10 messages\n",
      "   Bob: 8 messages\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from transformers import pipeline\n",
    "\n",
    "\"\"\" Rudimentary model for testing \"\"\"\n",
    "class GPT2Agent:\n",
    "    def __init__(self):\n",
    "        self.generator = pipeline(\"text-generation\", model=\"gpt2\", pad_token_id=50256)\n",
    "\n",
    "    \"\"\" Returning response based on query using gpt-2 model\"\"\"\n",
    "    \"\"\" More powerful models do this automatically \"\"\"\n",
    "    def invoke(self, messages):\n",
    "        # get the latest message\n",
    "        latest = messages[-1].content\n",
    "        \n",
    "        # simple prompt format that works with GPT-2\n",
    "        prompt = f\"User: {latest}\\nBot:\"\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_new_tokens=20,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=50256\n",
    "        )\n",
    "        \n",
    "        # extract response\n",
    "        generated = response[0]['generated_text']\n",
    "        bot_response = generated.split(\"Bot:\")[-1].strip()\n",
    "        \n",
    "        # clean up\n",
    "        if '\\n' in bot_response:\n",
    "            bot_response = bot_response.split('\\n')[0]\n",
    "        if not bot_response or len(bot_response) < 3:\n",
    "            bot_response = \"I understand.\"\n",
    "\n",
    "        return AIMessage(content=bot_response)\n",
    "\n",
    "\"\"\" Create the LangGraph setup \"\"\"\n",
    "def create_gpt2_agent():\n",
    "    model = GPT2Agent()\n",
    "\n",
    "    \"\"\" Processing engine \"\"\"\n",
    "    def agent_node(state: MessagesState):\n",
    "        \"\"\"LangGraph calls this with full conversation history\"\"\"\n",
    "        print(f\"Agent sees {len(state['messages'])} messages in memory:\")\n",
    "        for i, msg in enumerate(state[\"messages\"]):\n",
    "            role = \"Human\" if msg.type == \"human\" else \"AI\"\n",
    "            print(f\"   {i+1}. {role}: {msg.content}\")\n",
    "        \n",
    "        # AI model gets FULL conversation context\n",
    "        response = model.invoke(state[\"messages\"])\n",
    "\n",
    "        # returning new state\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    # build LangGraph workflow\n",
    "    workflow = StateGraph(MessagesState)        # like blueprint of agentic model\n",
    "    workflow.add_node(\"agent\", agent_node)      # add one processing unit\n",
    "    workflow.set_entry_point(\"agent\")           # start here\n",
    "    workflow.set_finish_point(\"agent\")          # end here\n",
    "    \n",
    "    # add memory persistence--separate from nodes!\n",
    "    memory = MemorySaver()\n",
    "    app = workflow.compile(checkpointer=memory) # connect memory to workflow\n",
    "    \n",
    "    return app\n",
    "\n",
    "\"\"\" Testing memory \"\"\"\n",
    "app = create_gpt2_agent()\n",
    "\n",
    "# CONVERSATION A: Alice's Chat\n",
    "print(\"=\" * 60)\n",
    "print(\"ALICE's CHAT\")\n",
    "print(\"=\" * 60)\n",
    "config_alice = {\"configurable\": {\"thread_id\": \"alice_chat\"}}\n",
    "\n",
    "print(\"Alice Turn 1:\")\n",
    "alice_1 = app.invoke(\n",
    "    {\"messages\": [HumanMessage(\"Hi, my name is Alice and I live in New York\")]}, \n",
    "    config_alice\n",
    ")\n",
    "\n",
    "print(\"\\nAlice Turn 2:\")\n",
    "alice_2 = app.invoke(\n",
    "    {\"messages\": alice_1[\"messages\"] + [HumanMessage(\"What's my name?\")]}, \n",
    "    config_alice\n",
    ")\n",
    "\n",
    "print(\"\\nAlice Turn 3:\")\n",
    "alice_3 = app.invoke(\n",
    "    {\"messages\": alice_2[\"messages\"] + [HumanMessage(\"Where do I live?\")]}, \n",
    "    config_alice\n",
    ")\n",
    "\n",
    "# CONVERSATION B: Bob's Chat (Different Thread)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"BOB'S CHAT\")\n",
    "print(\"=\" * 60)\n",
    "config_bob = {\"configurable\": {\"thread_id\": \"bob_chat\"}}\n",
    "\n",
    "print(\"Bob Turn 1:\")\n",
    "bob_1 = app.invoke(\n",
    "    {\"messages\": [HumanMessage(\"Hello, I'm Bob and I work as a teacher\")]}, \n",
    "    config_bob\n",
    ")\n",
    "\n",
    "print(\"\\nBob Turn 2:\")\n",
    "bob_2 = app.invoke(\n",
    "    {\"messages\": bob_1[\"messages\"] + [HumanMessage(\"What's my job?\")]}, \n",
    "    config_bob\n",
    ")\n",
    "\n",
    "print(\"\\nBob Turn 3:\")\n",
    "bob_3 = app.invoke(\n",
    "    {\"messages\": bob_2[\"messages\"] + [HumanMessage(\"Do you remember my name?\")]}, \n",
    "    config_bob\n",
    ")\n",
    "\n",
    "\n",
    "# RETURN TO ALICE'S CONVERSATION\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBACK TO ALICE'S CONVERSATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Alice Turn 4 (returning to her thread):\")\n",
    "alice_4 = app.invoke(\n",
    "    {\"messages\": alice_3[\"messages\"] + [HumanMessage(\"How many times have we talked?\")]}, \n",
    "    config_alice  # Same thread_id as Alice's original conversation\n",
    ")\n",
    "\n",
    "# CROSS-CONTAMINATION TEST\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCROSS-CONTAMINATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Alice asks about Bob (should NOT know Bob):\")\n",
    "alice_cross = app.invoke(\n",
    "    {\"messages\": alice_4[\"messages\"] + [HumanMessage(\"Do you know anyone named Bob?\")]}, \n",
    "    config_alice\n",
    ")\n",
    "\n",
    "print(\"\\nBob asks about Alice (should NOT know Alice):\")\n",
    "bob_cross = app.invoke(\n",
    "    {\"messages\": bob_3[\"messages\"] + [HumanMessage(\"Do you know anyone named Alice?\")]}, \n",
    "    config_bob\n",
    ")\n",
    "\n",
    "# FINAL SUMMARY\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CONVERSATION STATES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nAlice's conversation has {len(alice_cross['messages'])} messages:\")\n",
    "for i, msg in enumerate(alice_cross['messages'], 1):\n",
    "    role = \"Human\" if msg.type == \"human\" else \"AI\"\n",
    "    print(f\"   {i}. {role}: {msg.content[:50]}...\")\n",
    "\n",
    "print(f\"\\nBob's conversation has {len(bob_cross['messages'])} messages:\")\n",
    "for i, msg in enumerate(bob_cross['messages'], 1):\n",
    "    role = \"Human\" if msg.type == \"human\" else \"AI\"\n",
    "    print(f\"   {i}. {role}: {msg.content[:50]}...\")\n",
    "\n",
    "\n",
    "# KEY INSIGHTS\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKEY INSIGHTS FROM THIS TEST\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Each thread_id maintains completely separate conversation history\")\n",
    "print(\"2. Alice's conversation never knows about Bob's conversation\")\n",
    "print(\"3. Bob's conversation never knows about Alice's conversation\") \n",
    "print(\"4. You can return to any conversation thread and it remembers everything\")\n",
    "print(\"5. Each thread grows independently (different message counts)\")\n",
    "print(\"6. Thread isolation prevents data leakage between conversations\")\n",
    "\n",
    "print(f\"\\nCONVERSATION LENGTHS:\")\n",
    "print(f\"   Alice: {len(alice_cross['messages'])} messages\")\n",
    "print(f\"   Bob: {len(bob_cross['messages'])} messages\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b37da9-ea83-4e95-ac96-768a12ef2584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-agent)",
   "language": "python",
   "name": "lang-agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
